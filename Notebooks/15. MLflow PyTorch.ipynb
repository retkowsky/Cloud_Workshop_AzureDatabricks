{"cells":[{"cell_type":"markdown","source":["# Cloud Workshop Azure Databricks\n## 15. MLFlow avec PyTorch\n<img src=\"https://raw.githubusercontent.com/retkowsky/images/master/AzureDatabricksLogo.jpg\"><br>\nV1.3 12/06/2019"],"metadata":{}},{"cell_type":"markdown","source":["# MLflow PyTorch Notebook\n\nThis is an MLflow PyTorch notebook is based on [MLflow's PyTorch TensorBoard tutorial](https://github.com/mlflow/mlflow/blob/master/examples/pytorch/mnist_tensorboard_artifact.py).\n\n- This notebook demonstrates how to run PyTorch to fit a neural network on MNIST handwritten digit recognition data.\n- The run results are logged to an MLFlow server. \n- Training metrics and weights in TensorFlow event format are logged locally and then uploaded to the MLflow run's artifact directory.\n- TensorBoard is started on the local log and then optionally on the uploaded log.\n\nIn this tutorial you:\n\n- Create a **GPU-enabled cluster**\n- Install the MLflow library on the cluster\n- Run a neural network on MNIST handwritten digit recognition data\n- View the results of training the network in the MLflow experiment UI\n- View the results of training the network in TensorBoard"],"metadata":{}},{"cell_type":"markdown","source":["### Create a cluster and install MLflow on your cluster\n\n1. Create a GPU-enabled cluster specifying:\n    - **Databricks Runtime Version:** Databricks Runtime 5.0 ML Beta **GPU** or above\n    - **Python Version:** Python 3\n1. Install required library.\n   1. Create required library.\n    * Source **PyPI** and enter `mlflow`.\n   1. Install the library into the cluster.\n1. Attach this notebook to the cluster."],"metadata":{}},{"cell_type":"code","source":["import sys\nsys.version"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["## 1. Train an MNIST digit recognizer using PyTorch"],"metadata":{}},{"cell_type":"markdown","source":["#### Organize MLflow Runs into Experiments"],"metadata":{}},{"cell_type":"code","source":["# Trains using PyTorch and logs training metrics and weights in TensorFlow event format to the MLflow run's artifact directory. \n# This stores the TensorFlow events in MLflow for later access using TensorBoard.\n#\n# Code based on https://github.com/mlflow/mlflow/blob/master/example/tutorial/pytorch_tensorboard.py.\n\n\nfrom __future__ import print_function\nimport os\nimport tempfile\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\nfrom tensorboardX import SummaryWriter\nfrom collections import namedtuple\nimport tensorflow as tf\nimport tensorflow.summary\nfrom tensorflow.summary import scalar\nfrom tensorflow.summary import histogram\nfrom chardet.universaldetector import UniversalDetector\n\n# Create Params dictionary\nclass Params(object):\n\tdef __init__(self, batch_size, test_batch_size, epochs, lr, momentum, seed, cuda, log_interval):\n\t\tself.batch_size = batch_size\n\t\tself.test_batch_size = test_batch_size\n\t\tself.epochs = epochs\n\t\tself.lr = lr\n\t\tself.momentum = momentum\n\t\tself.seed = seed\n\t\tself.cuda = cuda\n\t\tself.log_interval = log_interval\n\n# Configure args\nargs = Params(64, 1000, 10, 0.01, 0.5, 1, True, 200)\n\ncuda = not args.cuda and torch.cuda.is_available()\n\n\nkwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('../data', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=args.batch_size, shuffle=True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=args.batch_size, shuffle=True, **kwargs)\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=0)\n\n    def log_weights(self, step):\n        writer.add_summary(histogram('weights/conv1/weight', model.conv1.weight.data).eval(), step)\n        writer.add_summary(histogram('weights/conv1/bias', model.conv1.bias.data).eval(), step)\n        writer.add_summary(histogram('weights/conv2/weight', model.conv2.weight.data).eval(), step)\n        writer.add_summary(histogram('weights/conv2/bias', model.conv2.bias.data).eval(), step)\n        writer.add_summary(histogram('weights/fc1/weight', model.fc1.weight.data).eval(), step)\n        writer.add_summary(histogram('weights/fc1/bias', model.fc1.bias.data).eval(), step)\n        writer.add_summary(histogram('weights/fc2/weight', model.fc2.weight.data).eval(), step)\n        writer.add_summary(histogram('weights/fc2/bias', model.fc2.bias.data).eval(), step)\n\nmodel = Model()\nif cuda:\n    model.cuda()\n\noptimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n\nwriter = None # Will be used to write TensorBoard events\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.data.item()))\n            step = epoch * len(train_loader) + batch_idx\n            log_scalar('train_loss', loss.data.item(), step)\n            model.log_weights(step)\n\ndef test(epoch):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            if cuda:\n                data, target = data.cuda(), target.cuda()\n            data, target = Variable(data), Variable(target)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').data.item() # sum up batch loss\n            pred = output.data.max(1)[1] # get the index of the max log-probability\n            correct += pred.eq(target.data).cpu().sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    test_accuracy = 100.0 * correct / len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset), test_accuracy))\n    step = (epoch + 1) * len(train_loader)\n    log_scalar('test_loss', test_loss, step)\n    log_scalar('test_accuracy', test_accuracy, step)\n\ndef log_scalar(name, value, step):\n    \"\"\"Log a scalar value to both MLflow and TensorBoard\"\"\"\n    writer.add_summary(scalar(name, value).eval(), step)\n    mlflow.log_metric(name, value)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["## 2. Create a TensorFlow session and start MLflow"],"metadata":{}},{"cell_type":"code","source":["import mlflow.pytorch\n\nsess = tf.InteractiveSession()\nwith mlflow.start_run() as run:  \n  # Log our parameters into mlflow\n  for key, value in vars(args).items():\n      mlflow.log_param(key, value)\n\n  output_dir = tempfile.mkdtemp()\n  print(\"Writing TensorFlow events locally to %s\\n\" % output_dir)\n  writer = tf.summary.FileWriter(output_dir, graph=sess.graph) \n\n  for epoch in range(1, args.epochs + 1):\n      # print out active_run\n      print(\"Active Run ID: %s, Epoch: %s \\n\" % (run.info.run_uuid, epoch))\n\n      train(epoch)\n      test(epoch)\n      \n  print(\"Uploading TensorFlow events as a run artifact.\")\n  mlflow.log_artifacts(output_dir, artifact_path=\"events\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Writing TensorFlow events locally to /tmp/tmpabd7f0_1\n\nActive Run ID: 9ac13f5dedf84afb86b3371959e77fdf, Epoch: 1 \n\nTrain Epoch: 1 [0/60000 (0%)]\tLoss: 4.197468\nTrain Epoch: 1 [12800/60000 (21%)]\tLoss: 3.365239\nTrain Epoch: 1 [25600/60000 (43%)]\tLoss: 2.860766\nTrain Epoch: 1 [38400/60000 (64%)]\tLoss: 2.725742\nTrain Epoch: 1 [51200/60000 (85%)]\tLoss: 2.662532\n\nTest set: Average loss: 2.2091, Accuracy: 9339/10000 (93%)\n\nActive Run ID: 9ac13f5dedf84afb86b3371959e77fdf, Epoch: 2 \n\nTrain Epoch: 2 [0/60000 (0%)]\tLoss: 2.606720\nTrain Epoch: 2 [12800/60000 (21%)]\tLoss: 2.556422\nTrain Epoch: 2 [25600/60000 (43%)]\tLoss: 2.501592\nTrain Epoch: 2 [38400/60000 (64%)]\tLoss: 2.630374\nTrain Epoch: 2 [51200/60000 (85%)]\tLoss: 2.420966\n\nTest set: Average loss: 2.0814, Accuracy: 9566/10000 (96%)\n\nActive Run ID: 9ac13f5dedf84afb86b3371959e77fdf, Epoch: 3 \n\nTrain Epoch: 3 [0/60000 (0%)]\tLoss: 2.290576\nTrain Epoch: 3 [12800/60000 (21%)]\tLoss: 2.216802\nTrain Epoch: 3 [25600/60000 (43%)]\tLoss: 2.456002\nTrain Epoch: 3 [38400/60000 (64%)]\tLoss: 2.397808\nTrain Epoch: 3 [51200/60000 (85%)]\tLoss: 2.426887\n\nTest set: Average loss: 2.0483, Accuracy: 9665/10000 (97%)\n\nActive Run ID: 9ac13f5dedf84afb86b3371959e77fdf, Epoch: 4 \n\nTrain Epoch: 4 [0/60000 (0%)]\tLoss: 2.225744\nTrain Epoch: 4 [12800/60000 (21%)]\tLoss: 2.489834\nTrain Epoch: 4 [25600/60000 (43%)]\tLoss: 2.139439\nTrain Epoch: 4 [38400/60000 (64%)]\tLoss: 2.164264\nTrain Epoch: 4 [51200/60000 (85%)]\tLoss: 2.166989\n\nTest set: Average loss: 2.0281, Accuracy: 9684/10000 (97%)\n\nActive Run ID: 9ac13f5dedf84afb86b3371959e77fdf, Epoch: 5 \n\nTrain Epoch: 5 [0/60000 (0%)]\tLoss: 2.098589\nTrain Epoch: 5 [12800/60000 (21%)]\tLoss: 2.046174\nTrain Epoch: 5 [25600/60000 (43%)]\tLoss: 2.181350\nTrain Epoch: 5 [38400/60000 (64%)]\tLoss: 2.194591\nTrain Epoch: 5 [51200/60000 (85%)]\tLoss: 2.279545\n\nTest set: Average loss: 2.0219, Accuracy: 9703/10000 (97%)\n\nActive Run ID: 9ac13f5dedf84afb86b3371959e77fdf, Epoch: 6 \n\nTrain Epoch: 6 [0/60000 (0%)]\tLoss: 2.260291\nTrain Epoch: 6 [12800/60000 (21%)]\tLoss: 2.120440\nTrain Epoch: 6 [25600/60000 (43%)]\tLoss: 2.242338\nTrain Epoch: 6 [38400/60000 (64%)]\tLoss: 2.151783\nTrain Epoch: 6 [51200/60000 (85%)]\tLoss: 2.193433\n\nTest set: Average loss: 2.0111, Accuracy: 9719/10000 (97%)\n\nActive Run ID: 9ac13f5dedf84afb86b3371959e77fdf, Epoch: 7 \n\nTrain Epoch: 7 [0/60000 (0%)]\tLoss: 2.162578\nTrain Epoch: 7 [12800/60000 (21%)]\tLoss: 2.197510\nTrain Epoch: 7 [25600/60000 (43%)]\tLoss: 2.198035\nTrain Epoch: 7 [38400/60000 (64%)]\tLoss: 2.074801\nTrain Epoch: 7 [51200/60000 (85%)]\tLoss: 2.224810\n\nTest set: Average loss: 2.0071, Accuracy: 9773/10000 (98%)\n\nActive Run ID: 9ac13f5dedf84afb86b3371959e77fdf, Epoch: 8 \n\nTrain Epoch: 8 [0/60000 (0%)]\tLoss: 2.086279\nTrain Epoch: 8 [12800/60000 (21%)]\tLoss: 2.084762\nTrain Epoch: 8 [25600/60000 (43%)]\tLoss: 2.158970\nTrain Epoch: 8 [38400/60000 (64%)]\tLoss: 2.214148\nTrain Epoch: 8 [51200/60000 (85%)]\tLoss: 2.097444\n\nTest set: Average loss: 2.0007, Accuracy: 9785/10000 (98%)\n\nActive Run ID: 9ac13f5dedf84afb86b3371959e77fdf, Epoch: 9 \n\nTrain Epoch: 9 [0/60000 (0%)]\tLoss: 2.211730\nTrain Epoch: 9 [12800/60000 (21%)]\tLoss: 2.040725\nTrain Epoch: 9 [25600/60000 (43%)]\tLoss: 2.116132\nTrain Epoch: 9 [38400/60000 (64%)]\tLoss: 2.141839\nTrain Epoch: 9 [51200/60000 (85%)]\tLoss: 2.048464\n\nTest set: Average loss: 2.0005, Accuracy: 9773/10000 (98%)\n\nActive Run ID: 9ac13f5dedf84afb86b3371959e77fdf, Epoch: 10 \n\nTrain Epoch: 10 [0/60000 (0%)]\tLoss: 2.145846\nTrain Epoch: 10 [12800/60000 (21%)]\tLoss: 2.121406\nTrain Epoch: 10 [25600/60000 (43%)]\tLoss: 2.185719\nTrain Epoch: 10 [38400/60000 (64%)]\tLoss: 2.078063\nTrain Epoch: 10 [51200/60000 (85%)]\tLoss: 2.064973\n\nTest set: Average loss: 1.9932, Accuracy: 9785/10000 (98%)\n\nUploading TensorFlow events as a run artifact.\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["## 3. Review the experiment using MLFlow integration with Azure Databricks"],"metadata":{}},{"cell_type":"markdown","source":["> Cliquer sur le bouton **Runs** en haut Ã  droite pour voir les runs MLFlow"],"metadata":{}},{"cell_type":"markdown","source":["#### MLflow UI for the PyTorch MNIST Run\n<img src=\"https://docs.databricks.com/_static/images/mlflow/mlflow-pytorch-mlflow-ui.gif\" width=1000/>"],"metadata":{}},{"cell_type":"markdown","source":["## 4. Start TensorBoard on local directory"],"metadata":{}},{"cell_type":"code","source":["dbutils.tensorboard.start(output_dir)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["\n<html>\n    <head>\n        <link rel=\"stylesheet\"type=\"text/css\"\n        href=\"https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css\" />\n    </head>\n    <body>\n        TensorBoard log directory set to: /tmp/tmpabd7f0_1. <a href=/driver-proxy/o/6420937752591893/0611-121228-frogs48/9009/>\n            View TensorBoard <i class=\"fa fa-external-link\"> </i>\n        </a>\n    </body>\n</html>\n"]}}],"execution_count":14},{"cell_type":"markdown","source":["### View the results in TensorBoard\n\nThe computations you'll use TensorFlow for - like training a massive deep neural network - can be complex and confusing. To make it easier to understand, debug, and optimize TensorFlow programs, we've included a suite of visualization tools called TensorBoard. You can use TensorBoard to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through it.\n\nhttps://www.tensorflow.org/guide/summaries_and_tensorboard\n\n\n<img src=\"https://docs.databricks.com/_static/images/third-party-integrations/tensorflow/tensorboard.png\"/>\n\nClick the **View TensorBoard** link. It should look like the following:"],"metadata":{}},{"cell_type":"markdown","source":["#### TensorBoard for the PyTorch MNIST Run\n<img src=\"https://docs.databricks.com/_static/images/mlflow/mlflow-pytorch-tensorboard.gif\" width=1000/>"],"metadata":{}},{"cell_type":"markdown","source":["## Stop TensorBoard"],"metadata":{}},{"cell_type":"code","source":["dbutils.tensorboard.stop()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Looking for active tensorboard process...\nActive tensorboard process killed...\nOut[5]: True\n</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["> Fin"],"metadata":{}}],"metadata":{"name":"4 MLflow PyTorch","notebookId":1299617962160877},"nbformat":4,"nbformat_minor":0}
